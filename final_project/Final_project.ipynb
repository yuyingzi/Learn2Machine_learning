{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project DSCI-552\n",
    "\n",
    "\n",
    "reference: \n",
    "1) https://matplotlib.org/stable/index.html\n",
    "2) https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "3) https://github.com/INTERMT/Awesome-Keras-Chinese"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) In this problem, we are trying to build a classifier to analyze the sentiment of\n",
    "reviews. You are provided with text data in two folders: one folder involves\n",
    "positive reviews, and one folder involves negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_txt_files(directory):\n",
    "    texts = []\n",
    "    file_paths = glob.glob(os.path.join(directory, '*.txt'))\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        return ''  # debug\n",
    "    # remove non-alphabetical characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def split(df):\n",
    "    # iii. The name of each text file starts with cv number. \n",
    "    # Use text files 0-699 in each class for training and 700-999 for testing\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    training_df = df.iloc[:700, :]\n",
    "    testing_df = df.iloc[700:, :]\n",
    "    return training_df, testing_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_directory = '/Users/yinterested/文件(D)/研究生课程/DSCI_552-master/final_project/Data/neg/'\n",
    "pos_directory = '/Users/yinterested/文件(D)/研究生课程/DSCI_552-master/final_project/Data/pos/'\n",
    "\n",
    "neg_labels = [-1] * len(read_txt_files(neg_directory))  # negative label\n",
    "pos_labels = [1] * len(read_txt_files(pos_directory))  # positive label\n",
    "\n",
    "neg_df = pd.DataFrame({'text': read_txt_files(neg_directory), 'label': neg_labels})\n",
    "pos_df = pd.DataFrame({'text': read_txt_files(neg_directory), 'label': pos_labels})\n",
    "\n",
    "training_dfneg, testing_dfneg = split(neg_df)\n",
    "training_dfpos, testing_dfpos = split(pos_df)\n",
    "\n",
    "# Combine the two dataframe into 1\n",
    "training = pd.concat([training_dfneg, training_dfpos], ignore_index=True)\n",
    "testing = pd.concat([testing_dfneg, testing_dfpos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad bad bad that one word seems to pretty much...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isnt it the ultimate sign of a movies cinemati...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gordy is not a movie it is a minutelong sesame...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disconnect the phone line dont accept the char...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>ahh yes the teenage romance an attractive youn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>because im a scientist thats what we do dr ale...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>written by david j schow and john shirley base...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>one of my brothers favorite movies is h b hali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>the premise of this movie is well pretty farfe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     bad bad bad that one word seems to pretty much...     -1\n",
       "1     isnt it the ultimate sign of a movies cinemati...     -1\n",
       "2     gordy is not a movie it is a minutelong sesame...     -1\n",
       "3     disconnect the phone line dont accept the char...     -1\n",
       "4     when robert forster found himself famous again...     -1\n",
       "...                                                 ...    ...\n",
       "1395  ahh yes the teenage romance an attractive youn...      1\n",
       "1396  because im a scientist thats what we do dr ale...      1\n",
       "1397  written by david j schow and john shirley base...      1\n",
       "1398  one of my brothers favorite movies is h b hali...      1\n",
       "1399  the premise of this movie is well pretty farfe...      1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synopsis cromagnon ayla loses her mother to an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contrary to popular belief not every single fo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and i thought stigmata would be the worst reli...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>years ago national lampoon introduced us to a ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roger ebert asks in his review of sexy beast w...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>synopsis when a meteorite crashlands in the ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>its now the anniversary of the slayings of jul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>coinciding with the emerging popularity of mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>and now the highflying hong kong style of film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>battlefield long boring and just plain stupid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    synopsis cromagnon ayla loses her mother to an...     -1\n",
       "1    contrary to popular belief not every single fo...     -1\n",
       "2    and i thought stigmata would be the worst reli...     -1\n",
       "3    years ago national lampoon introduced us to a ...     -1\n",
       "4    roger ebert asks in his review of sexy beast w...     -1\n",
       "..                                                 ...    ...\n",
       "595  synopsis when a meteorite crashlands in the ar...      1\n",
       "596  its now the anniversary of the slayings of jul...      1\n",
       "597  coinciding with the emerging popularity of mov...      1\n",
       "598  and now the highflying hong kong style of film...      1\n",
       "599  battlefield long boring and just plain stupid ...      1\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad bad bad that one word seems to pretty much...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isnt it the ultimate sign of a movies cinemati...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gordy is not a movie it is a minutelong sesame...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disconnect the phone line dont accept the char...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>synopsis when a meteorite crashlands in the ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>its now the anniversary of the slayings of jul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>coinciding with the emerging popularity of mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>and now the highflying hong kong style of film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>battlefield long boring and just plain stupid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    bad bad bad that one word seems to pretty much...     -1\n",
       "1    isnt it the ultimate sign of a movies cinemati...     -1\n",
       "2    gordy is not a movie it is a minutelong sesame...     -1\n",
       "3    disconnect the phone line dont accept the char...     -1\n",
       "4    when robert forster found himself famous again...     -1\n",
       "..                                                 ...    ...\n",
       "595  synopsis when a meteorite crashlands in the ar...      1\n",
       "596  its now the anniversary of the slayings of jul...      1\n",
       "597  coinciding with the emerging popularity of mov...      1\n",
       "598  and now the highflying hong kong style of film...      1\n",
       "599  battlefield long boring and just plain stupid ...      1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([training,testing])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Count the number of unique words in the whole dataset (train + test) and\n",
    "print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique words in the whole dataset is 32186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True) # set binary true to only count each word once\n",
    "text = df['text'].to_list()\n",
    "vectorizer.fit(text)\n",
    "unique_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print\n",
    "print(\"the number of unique words in the whole dataset is\", len(unique_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Calculate the average review length and the standard deviation of review\n",
    "lengths. Report the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average review length is 607.502, the standard deviation (SD) of review length is 255.77627870538385\n"
     ]
    }
   ],
   "source": [
    "# Calculate average review length, split word, not count the string lenth\n",
    "df['review_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "average_length = df['review_length'].mean()\n",
    "std_dev = df['review_length'].std()\n",
    "\n",
    "print(f\"The average review length is {average_length}, the standard deviation (SD) of review length is {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad bad bad that one word seems to pretty much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isnt it the ultimate sign of a movies cinemati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gordy is not a movie it is a minutelong sesame...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disconnect the phone line dont accept the char...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>synopsis when a meteorite crashlands in the ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>its now the anniversary of the slayings of jul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>coinciding with the emerging popularity of mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>and now the highflying hong kong style of film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>battlefield long boring and just plain stupid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    bad bad bad that one word seems to pretty much...      0\n",
       "1    isnt it the ultimate sign of a movies cinemati...      0\n",
       "2    gordy is not a movie it is a minutelong sesame...      0\n",
       "3    disconnect the phone line dont accept the char...      0\n",
       "4    when robert forster found himself famous again...      0\n",
       "..                                                 ...    ...\n",
       "595  synopsis when a meteorite crashlands in the ar...      1\n",
       "596  its now the anniversary of the slayings of jul...      1\n",
       "597  coinciding with the emerging popularity of mov...      1\n",
       "598  and now the highflying hong kong style of film...      1\n",
       "599  battlefield long boring and just plain stupid ...      1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi. Plot the histogram of review lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the number of bins with the *bins* keyword argument as 50, \n",
    "# the review lenth is obviously a skewed distribution\n",
    "df['review_length'].hist(bins=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vii. To represent each text (= data point), there are many ways. In NLP/Deep\n",
    "Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word\n",
    "in the text will be represented as 1, the second most common word will be\n",
    "represented as 2, etc. Tokenize each text document using this method\n",
    "\n",
    "reference: \n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_TFCO_Wiki_Case_Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n"
     ]
    }
   ],
   "source": [
    "# pip install apache_beam\n",
    "# pip install --upgrade keras-nlp\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "import apache_beam as beam\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'and': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'in': 7,\n",
       " 'that': 8,\n",
       " 'it': 9,\n",
       " 'with': 10,\n",
       " 'this': 11,\n",
       " 'as': 12,\n",
       " 'for': 13,\n",
       " 'but': 14,\n",
       " 'i': 15,\n",
       " 'film': 16,\n",
       " 'his': 17,\n",
       " 'on': 18,\n",
       " 'he': 19,\n",
       " 'are': 20,\n",
       " 'be': 21,\n",
       " 'movie': 22,\n",
       " 'its': 23,\n",
       " 'by': 24,\n",
       " 'an': 25,\n",
       " 'have': 26,\n",
       " 'not': 27,\n",
       " 'one': 28,\n",
       " 'at': 29,\n",
       " 'who': 30,\n",
       " 'was': 31,\n",
       " 'from': 32,\n",
       " 'you': 33,\n",
       " 'they': 34,\n",
       " 'has': 35,\n",
       " 'all': 36,\n",
       " 'her': 37,\n",
       " 'so': 38,\n",
       " 'like': 39,\n",
       " 'about': 40,\n",
       " 'out': 41,\n",
       " 'or': 42,\n",
       " 'when': 43,\n",
       " 'up': 44,\n",
       " 'just': 45,\n",
       " 'if': 46,\n",
       " 'some': 47,\n",
       " 'more': 48,\n",
       " 'what': 49,\n",
       " 'there': 50,\n",
       " 'their': 51,\n",
       " 'which': 52,\n",
       " 'no': 53,\n",
       " 'even': 54,\n",
       " 'only': 55,\n",
       " 'into': 56,\n",
       " 'she': 57,\n",
       " 'him': 58,\n",
       " 'than': 59,\n",
       " 'good': 60,\n",
       " 'time': 61,\n",
       " 'we': 62,\n",
       " 'can': 63,\n",
       " 'been': 64,\n",
       " 'would': 65,\n",
       " 'get': 66,\n",
       " 'do': 67,\n",
       " 'bad': 68,\n",
       " 'much': 69,\n",
       " 'will': 70,\n",
       " 'them': 71,\n",
       " 'most': 72,\n",
       " 'characters': 73,\n",
       " 'films': 74,\n",
       " 'story': 75,\n",
       " 'character': 76,\n",
       " 'any': 77,\n",
       " 'plot': 78,\n",
       " 'other': 79,\n",
       " 'two': 80,\n",
       " 'after': 81,\n",
       " 'had': 82,\n",
       " 'make': 83,\n",
       " 'too': 84,\n",
       " 'because': 85,\n",
       " 'first': 86,\n",
       " 'really': 87,\n",
       " 'off': 88,\n",
       " 'then': 89,\n",
       " 'see': 90,\n",
       " 'how': 91,\n",
       " 'could': 92,\n",
       " 'also': 93,\n",
       " 'way': 94,\n",
       " 'me': 95,\n",
       " 'my': 96,\n",
       " 'very': 97,\n",
       " 'well': 98,\n",
       " 'were': 99,\n",
       " 'little': 100,\n",
       " 'does': 101,\n",
       " 'where': 102,\n",
       " 'movies': 103,\n",
       " 'dont': 104,\n",
       " 'doesnt': 105,\n",
       " 'while': 106,\n",
       " 'scene': 107,\n",
       " 'people': 108,\n",
       " 'never': 109,\n",
       " 'know': 110,\n",
       " 'over': 111,\n",
       " 'scenes': 112,\n",
       " 'being': 113,\n",
       " 'action': 114,\n",
       " 'these': 115,\n",
       " 'here': 116,\n",
       " 'theres': 117,\n",
       " 'hes': 118,\n",
       " 'man': 119,\n",
       " 'such': 120,\n",
       " 'director': 121,\n",
       " 'why': 122,\n",
       " 'new': 123,\n",
       " 'through': 124,\n",
       " 'another': 125,\n",
       " 'big': 126,\n",
       " 'made': 127,\n",
       " 'go': 128,\n",
       " 'better': 129,\n",
       " 'end': 130,\n",
       " 'something': 131,\n",
       " 'seems': 132,\n",
       " 's': 133,\n",
       " 'should': 134,\n",
       " 'every': 135,\n",
       " 'work': 136,\n",
       " 'nothing': 137,\n",
       " 'best': 138,\n",
       " 'isnt': 139,\n",
       " 'many': 140,\n",
       " 'life': 141,\n",
       " 'us': 142,\n",
       " 'before': 143,\n",
       " 'enough': 144,\n",
       " 'now': 145,\n",
       " 'back': 146,\n",
       " 'those': 147,\n",
       " 'few': 148,\n",
       " 'script': 149,\n",
       " 'around': 150,\n",
       " 'going': 151,\n",
       " 'think': 152,\n",
       " 'audience': 153,\n",
       " 'still': 154,\n",
       " 'thing': 155,\n",
       " 'love': 156,\n",
       " 'funny': 157,\n",
       " 'gets': 158,\n",
       " 'down': 159,\n",
       " 'actually': 160,\n",
       " 'look': 161,\n",
       " 'thats': 162,\n",
       " 'makes': 163,\n",
       " 'years': 164,\n",
       " 'though': 165,\n",
       " 'take': 166,\n",
       " 'did': 167,\n",
       " 'however': 168,\n",
       " 'last': 169,\n",
       " 'comedy': 170,\n",
       " 'least': 171,\n",
       " 'your': 172,\n",
       " 'real': 173,\n",
       " 'things': 174,\n",
       " 'say': 175,\n",
       " 'between': 176,\n",
       " 'same': 177,\n",
       " 'great': 178,\n",
       " 'fact': 179,\n",
       " 'played': 180,\n",
       " 'minutes': 181,\n",
       " 'since': 182,\n",
       " 'role': 183,\n",
       " 'seen': 184,\n",
       " 'plays': 185,\n",
       " 'point': 186,\n",
       " 'actors': 187,\n",
       " 'long': 188,\n",
       " 'cant': 189,\n",
       " 'acting': 190,\n",
       " 'im': 191,\n",
       " 'come': 192,\n",
       " 'find': 193,\n",
       " 'almost': 194,\n",
       " 'cast': 195,\n",
       " 'guy': 196,\n",
       " 'may': 197,\n",
       " 'comes': 198,\n",
       " 'old': 199,\n",
       " 'original': 200,\n",
       " 'john': 201,\n",
       " 'world': 202,\n",
       " 'right': 203,\n",
       " 'ever': 204,\n",
       " 'show': 205,\n",
       " 'goes': 206,\n",
       " 'anything': 207,\n",
       " 'performance': 208,\n",
       " 'might': 209,\n",
       " 'course': 210,\n",
       " 'interesting': 211,\n",
       " 'part': 212,\n",
       " 'didnt': 213,\n",
       " 'lot': 214,\n",
       " 'effects': 215,\n",
       " 'both': 216,\n",
       " 'again': 217,\n",
       " 'trying': 218,\n",
       " 'give': 219,\n",
       " 'away': 220,\n",
       " 'own': 221,\n",
       " 'although': 222,\n",
       " 'without': 223,\n",
       " 'young': 224,\n",
       " 'dialogue': 225,\n",
       " 'series': 226,\n",
       " 'want': 227,\n",
       " 'three': 228,\n",
       " 'far': 229,\n",
       " 'screen': 230,\n",
       " 'must': 231,\n",
       " 'pretty': 232,\n",
       " 'himself': 233,\n",
       " 'watch': 234,\n",
       " 'once': 235,\n",
       " 'instead': 236,\n",
       " 'during': 237,\n",
       " 'given': 238,\n",
       " 'having': 239,\n",
       " 'looks': 240,\n",
       " 'special': 241,\n",
       " 'rather': 242,\n",
       " 'reason': 243,\n",
       " 'star': 244,\n",
       " 'our': 245,\n",
       " 'got': 246,\n",
       " 'each': 247,\n",
       " 'takes': 248,\n",
       " 'place': 249,\n",
       " 'seem': 250,\n",
       " 'day': 251,\n",
       " 'kind': 252,\n",
       " 'year': 253,\n",
       " 'whole': 254,\n",
       " 'watching': 255,\n",
       " 'hard': 256,\n",
       " 'money': 257,\n",
       " 'worst': 258,\n",
       " 'probably': 259,\n",
       " 'sure': 260,\n",
       " 'woman': 261,\n",
       " 'theyre': 262,\n",
       " 'becomes': 263,\n",
       " 'unfortunately': 264,\n",
       " 'hollywood': 265,\n",
       " 'along': 266,\n",
       " 'set': 267,\n",
       " 'making': 268,\n",
       " 'actor': 269,\n",
       " 'fun': 270,\n",
       " 'times': 271,\n",
       " 'help': 272,\n",
       " 'bit': 273,\n",
       " 'wife': 274,\n",
       " 'quite': 275,\n",
       " 'supposed': 276,\n",
       " 'sex': 277,\n",
       " 'home': 278,\n",
       " 'wants': 279,\n",
       " 'problem': 280,\n",
       " 'everything': 281,\n",
       " 'anyone': 282,\n",
       " 'sense': 283,\n",
       " 'completely': 284,\n",
       " 'rest': 285,\n",
       " 'picture': 286,\n",
       " 'looking': 287,\n",
       " 'yet': 288,\n",
       " 'high': 289,\n",
       " 'simply': 290,\n",
       " 'next': 291,\n",
       " 'couple': 292,\n",
       " 'tries': 293,\n",
       " 'shes': 294,\n",
       " 'maybe': 295,\n",
       " 'boring': 296,\n",
       " 'always': 297,\n",
       " 'lost': 298,\n",
       " 'together': 299,\n",
       " 'girl': 300,\n",
       " 'either': 301,\n",
       " 'else': 302,\n",
       " 'idea': 303,\n",
       " 'someone': 304,\n",
       " 'name': 305,\n",
       " 'humor': 306,\n",
       " 'case': 307,\n",
       " 'wrong': 308,\n",
       " 'thought': 309,\n",
       " 'stupid': 310,\n",
       " 'less': 311,\n",
       " 'getting': 312,\n",
       " 'play': 313,\n",
       " 'black': 314,\n",
       " 'family': 315,\n",
       " 'men': 316,\n",
       " 'everyone': 317,\n",
       " 'job': 318,\n",
       " 'become': 319,\n",
       " 'james': 320,\n",
       " 'line': 321,\n",
       " 'put': 322,\n",
       " 'half': 323,\n",
       " 'called': 324,\n",
       " 'youre': 325,\n",
       " 'feel': 326,\n",
       " 'main': 327,\n",
       " 'moments': 328,\n",
       " 'tv': 329,\n",
       " 'michael': 330,\n",
       " 'perhaps': 331,\n",
       " 'done': 332,\n",
       " 'music': 333,\n",
       " 'evil': 334,\n",
       " 'stars': 335,\n",
       " 'death': 336,\n",
       " 'night': 337,\n",
       " 'left': 338,\n",
       " 'used': 339,\n",
       " 'said': 340,\n",
       " 'turns': 341,\n",
       " 'lines': 342,\n",
       " 'house': 343,\n",
       " 'whose': 344,\n",
       " 'city': 345,\n",
       " 'until': 346,\n",
       " 'under': 347,\n",
       " 'dead': 348,\n",
       " 'gives': 349,\n",
       " 'doing': 350,\n",
       " 'itself': 351,\n",
       " 'school': 352,\n",
       " 'entire': 353,\n",
       " 'found': 354,\n",
       " 'horror': 355,\n",
       " 'wasnt': 356,\n",
       " 'soon': 357,\n",
       " 'car': 358,\n",
       " 'running': 359,\n",
       " 'later': 360,\n",
       " 'head': 361,\n",
       " 'obvious': 362,\n",
       " 'american': 363,\n",
       " 'mind': 364,\n",
       " 'use': 365,\n",
       " 'days': 366,\n",
       " 'written': 367,\n",
       " 'shows': 368,\n",
       " 'video': 369,\n",
       " 'care': 370,\n",
       " 'attempt': 371,\n",
       " 'group': 372,\n",
       " 'ive': 373,\n",
       " 'keep': 374,\n",
       " 'friend': 375,\n",
       " 'worse': 376,\n",
       " 'hand': 377,\n",
       " 'several': 378,\n",
       " 'need': 379,\n",
       " 'save': 380,\n",
       " 'friends': 381,\n",
       " 'fight': 382,\n",
       " 'kevin': 383,\n",
       " 'based': 384,\n",
       " 'second': 385,\n",
       " 'comic': 386,\n",
       " 'certainly': 387,\n",
       " 'ending': 388,\n",
       " 'camera': 389,\n",
       " 'jokes': 390,\n",
       " 'sequence': 391,\n",
       " 'start': 392,\n",
       " 'kill': 393,\n",
       " 'kids': 394,\n",
       " 'father': 395,\n",
       " 'book': 396,\n",
       " 'summer': 397,\n",
       " 'especially': 398,\n",
       " 'moment': 399,\n",
       " 'mr': 400,\n",
       " 'review': 401,\n",
       " 'daughter': 402,\n",
       " 'guys': 403,\n",
       " 'playing': 404,\n",
       " 'believe': 405,\n",
       " 'none': 406,\n",
       " 'team': 407,\n",
       " 'already': 408,\n",
       " 'tell': 409,\n",
       " 'human': 410,\n",
       " 'hour': 411,\n",
       " 'named': 412,\n",
       " 'turn': 413,\n",
       " 'problems': 414,\n",
       " 'sequences': 415,\n",
       " 'nice': 416,\n",
       " 'small': 417,\n",
       " 'starts': 418,\n",
       " 'finally': 419,\n",
       " 'example': 420,\n",
       " 'directed': 421,\n",
       " 'whos': 422,\n",
       " 'begins': 423,\n",
       " 'david': 424,\n",
       " 'try': 425,\n",
       " 'performances': 426,\n",
       " 'let': 427,\n",
       " 'final': 428,\n",
       " 'early': 429,\n",
       " 'earth': 430,\n",
       " 'happens': 431,\n",
       " 'batman': 432,\n",
       " 'hit': 433,\n",
       " 'screenplay': 434,\n",
       " 'face': 435,\n",
       " 'sort': 436,\n",
       " 'guess': 437,\n",
       " 'poor': 438,\n",
       " 'town': 439,\n",
       " 'hell': 440,\n",
       " 'opening': 441,\n",
       " 'worth': 442,\n",
       " 'whats': 443,\n",
       " 'title': 444,\n",
       " 'matter': 445,\n",
       " 'shot': 446,\n",
       " 'material': 447,\n",
       " 'different': 448,\n",
       " 'seeing': 449,\n",
       " 'space': 450,\n",
       " 'behind': 451,\n",
       " 'run': 452,\n",
       " 'style': 453,\n",
       " 'live': 454,\n",
       " 'full': 455,\n",
       " 'arent': 456,\n",
       " 'against': 457,\n",
       " 'past': 458,\n",
       " 'boy': 459,\n",
       " 'girls': 460,\n",
       " 'appears': 461,\n",
       " 'attempts': 462,\n",
       " 'saw': 463,\n",
       " 'thriller': 464,\n",
       " 'game': 465,\n",
       " 'yes': 466,\n",
       " 'top': 467,\n",
       " 'nearly': 468,\n",
       " 'question': 469,\n",
       " 'women': 470,\n",
       " 'exactly': 471,\n",
       " 'person': 472,\n",
       " 'wild': 473,\n",
       " 'five': 474,\n",
       " 'mother': 475,\n",
       " 'filmmakers': 476,\n",
       " 'major': 477,\n",
       " 'talent': 478,\n",
       " 'including': 479,\n",
       " 'despite': 480,\n",
       " 'deep': 481,\n",
       " 'robin': 482,\n",
       " 'alien': 483,\n",
       " 'able': 484,\n",
       " 'falls': 485,\n",
       " 'relationship': 486,\n",
       " 'finds': 487,\n",
       " 'planet': 488,\n",
       " 'ends': 489,\n",
       " 'upon': 490,\n",
       " 'paul': 491,\n",
       " 'short': 492,\n",
       " 'career': 493,\n",
       " 'works': 494,\n",
       " 'lives': 495,\n",
       " 'hope': 496,\n",
       " 'son': 497,\n",
       " 'roles': 498,\n",
       " 'joe': 499,\n",
       " 'quickly': 500,\n",
       " 'version': 501,\n",
       " 'wonder': 502,\n",
       " 'often': 503,\n",
       " 'act': 504,\n",
       " 'apparently': 505,\n",
       " 'production': 506,\n",
       " 'killer': 507,\n",
       " 'mission': 508,\n",
       " 'lee': 509,\n",
       " 'except': 510,\n",
       " 'body': 511,\n",
       " 'says': 512,\n",
       " 'coming': 513,\n",
       " 'violence': 514,\n",
       " 'lack': 515,\n",
       " 'obviously': 516,\n",
       " 'entertaining': 517,\n",
       " 'mess': 518,\n",
       " 'eyes': 519,\n",
       " 'laugh': 520,\n",
       " 'possible': 521,\n",
       " 'peter': 522,\n",
       " 'jack': 523,\n",
       " 'anyway': 524,\n",
       " 'stop': 525,\n",
       " 'york': 526,\n",
       " 'interest': 527,\n",
       " 'brothers': 528,\n",
       " 'mean': 529,\n",
       " 'basically': 530,\n",
       " 'wont': 531,\n",
       " 'wouldnt': 532,\n",
       " 'side': 533,\n",
       " 'hero': 534,\n",
       " 'killed': 535,\n",
       " 'knows': 536,\n",
       " 'level': 537,\n",
       " 'room': 538,\n",
       " 'children': 539,\n",
       " 'annoying': 540,\n",
       " 'predictable': 541,\n",
       " 'beginning': 542,\n",
       " 'waste': 543,\n",
       " 'order': 544,\n",
       " 'direction': 545,\n",
       " 'dr': 546,\n",
       " 'seemed': 547,\n",
       " 'van': 548,\n",
       " 'mostly': 549,\n",
       " 'oh': 550,\n",
       " 'fails': 551,\n",
       " 'alone': 552,\n",
       " 'needs': 553,\n",
       " 'true': 554,\n",
       " 'drama': 555,\n",
       " 'robert': 556,\n",
       " 'late': 557,\n",
       " 'white': 558,\n",
       " 'bunch': 559,\n",
       " 'sequel': 560,\n",
       " 'youll': 561,\n",
       " 'stuff': 562,\n",
       " 'involving': 563,\n",
       " 'ridiculous': 564,\n",
       " 'hours': 565,\n",
       " 'happen': 566,\n",
       " 'lead': 567,\n",
       " 'involved': 568,\n",
       " 'themselves': 569,\n",
       " 'godzilla': 570,\n",
       " 'talk': 571,\n",
       " 'known': 572,\n",
       " 'cool': 573,\n",
       " 'murder': 574,\n",
       " 'television': 575,\n",
       " 'police': 576,\n",
       " 'project': 577,\n",
       " 'harry': 578,\n",
       " 'romantic': 579,\n",
       " 'fans': 580,\n",
       " 'terrible': 581,\n",
       " 'four': 582,\n",
       " 'am': 583,\n",
       " 'manages': 584,\n",
       " 'middle': 585,\n",
       " 'giving': 586,\n",
       " 'credits': 587,\n",
       " 'serious': 588,\n",
       " 'ship': 589,\n",
       " 'fine': 590,\n",
       " 'laughs': 591,\n",
       " 'scream': 592,\n",
       " 'close': 593,\n",
       " 'bring': 594,\n",
       " 'truly': 595,\n",
       " 'supporting': 596,\n",
       " 'complete': 597,\n",
       " 'premise': 598,\n",
       " 'single': 599,\n",
       " 'awful': 600,\n",
       " 'figure': 601,\n",
       " 'result': 602,\n",
       " 'king': 603,\n",
       " 'fall': 604,\n",
       " 'dull': 605,\n",
       " 'couldnt': 606,\n",
       " 'word': 607,\n",
       " 'nor': 608,\n",
       " 'piece': 609,\n",
       " 'across': 610,\n",
       " 'dumb': 611,\n",
       " 'classic': 612,\n",
       " 'suspense': 613,\n",
       " 'west': 614,\n",
       " 'sound': 615,\n",
       " 'brother': 616,\n",
       " 'taking': 617,\n",
       " 'villain': 618,\n",
       " 'tells': 619,\n",
       " 'throughout': 620,\n",
       " 'meet': 621,\n",
       " 'theater': 622,\n",
       " 'minute': 623,\n",
       " 'mars': 624,\n",
       " 'straight': 625,\n",
       " 'within': 626,\n",
       " 'seriously': 627,\n",
       " 'chance': 628,\n",
       " 'features': 629,\n",
       " 'dog': 630,\n",
       " 'scary': 631,\n",
       " 'smith': 632,\n",
       " 'girlfriend': 633,\n",
       " 'potential': 634,\n",
       " 'note': 635,\n",
       " 'ill': 636,\n",
       " 'crew': 637,\n",
       " 'elements': 638,\n",
       " 'went': 639,\n",
       " 'number': 640,\n",
       " 'others': 641,\n",
       " 'tom': 642,\n",
       " 'subplot': 643,\n",
       " 'somehow': 644,\n",
       " 'writing': 645,\n",
       " 'parts': 646,\n",
       " 'thinking': 647,\n",
       " 'particularly': 648,\n",
       " 'god': 649,\n",
       " 'decent': 650,\n",
       " 'words': 651,\n",
       " 'add': 652,\n",
       " 'wasted': 653,\n",
       " 'call': 654,\n",
       " 'talking': 655,\n",
       " 'taken': 656,\n",
       " 'battle': 657,\n",
       " 'id': 658,\n",
       " 'presence': 659,\n",
       " 'quality': 660,\n",
       " 'myself': 661,\n",
       " 'simple': 662,\n",
       " 'genre': 663,\n",
       " 'living': 664,\n",
       " 'williams': 665,\n",
       " 'lets': 666,\n",
       " 'usually': 667,\n",
       " 'neither': 668,\n",
       " 'gone': 669,\n",
       " 'novel': 670,\n",
       " 'dark': 671,\n",
       " 'recent': 672,\n",
       " 'de': 673,\n",
       " 'chris': 674,\n",
       " 'flick': 675,\n",
       " 'giant': 676,\n",
       " 'starring': 677,\n",
       " 'ones': 678,\n",
       " 'die': 679,\n",
       " 'sets': 680,\n",
       " 'cut': 681,\n",
       " 'turned': 682,\n",
       " 'william': 683,\n",
       " 'type': 684,\n",
       " 'absolutely': 685,\n",
       " 'came': 686,\n",
       " 'water': 687,\n",
       " 'ago': 688,\n",
       " 'begin': 689,\n",
       " 'expect': 690,\n",
       " 'shots': 691,\n",
       " 'beautiful': 692,\n",
       " 'eventually': 693,\n",
       " 'feels': 694,\n",
       " 'feeling': 695,\n",
       " 'surprise': 696,\n",
       " 'forced': 697,\n",
       " 'remember': 698,\n",
       " 'read': 699,\n",
       " 'working': 700,\n",
       " 'ten': 701,\n",
       " 'heart': 702,\n",
       " 'meets': 703,\n",
       " 'perfect': 704,\n",
       " 'among': 705,\n",
       " 'cop': 706,\n",
       " 'huge': 707,\n",
       " 'sexual': 708,\n",
       " 'child': 709,\n",
       " 'leave': 710,\n",
       " 'plan': 711,\n",
       " 'decides': 712,\n",
       " 'emotional': 713,\n",
       " 'scott': 714,\n",
       " 'power': 715,\n",
       " 'attention': 716,\n",
       " 'silly': 717,\n",
       " 'war': 718,\n",
       " 'future': 719,\n",
       " 'move': 720,\n",
       " 'jim': 721,\n",
       " 'computer': 722,\n",
       " 'youve': 723,\n",
       " 'success': 724,\n",
       " 'party': 725,\n",
       " 'lame': 726,\n",
       " 'pay': 727,\n",
       " 'writer': 728,\n",
       " 'leads': 729,\n",
       " 'saying': 730,\n",
       " 'studio': 731,\n",
       " 'mystery': 732,\n",
       " 'experience': 733,\n",
       " 'runs': 734,\n",
       " 'feature': 735,\n",
       " 'latest': 736,\n",
       " 'easy': 737,\n",
       " 'wait': 738,\n",
       " 'enjoy': 739,\n",
       " 'extremely': 740,\n",
       " 'female': 741,\n",
       " 'difficult': 742,\n",
       " 'certain': 743,\n",
       " 'steve': 744,\n",
       " 'business': 745,\n",
       " 'popular': 746,\n",
       " 'sometimes': 747,\n",
       " 'jones': 748,\n",
       " 'impressive': 749,\n",
       " 'deal': 750,\n",
       " 'fan': 751,\n",
       " 'return': 752,\n",
       " 'joke': 753,\n",
       " 'rock': 754,\n",
       " 'crime': 755,\n",
       " 'actress': 756,\n",
       " 'local': 757,\n",
       " 'release': 758,\n",
       " 'writers': 759,\n",
       " 'office': 760,\n",
       " 'impossible': 761,\n",
       " 'murphy': 762,\n",
       " 'beyond': 763,\n",
       " 'following': 764,\n",
       " 'former': 765,\n",
       " 'ideas': 766,\n",
       " 'cheap': 767,\n",
       " 'parents': 768,\n",
       " 'score': 769,\n",
       " 'eddie': 770,\n",
       " 'whether': 771,\n",
       " 'gun': 772,\n",
       " 'chase': 773,\n",
       " 'viewers': 774,\n",
       " 'george': 775,\n",
       " 'form': 776,\n",
       " 'wanted': 777,\n",
       " 'kid': 778,\n",
       " 'richard': 779,\n",
       " 'message': 780,\n",
       " 'bob': 781,\n",
       " 'talented': 782,\n",
       " 'tim': 783,\n",
       " 'entertainment': 784,\n",
       " 'leaves': 785,\n",
       " 'voice': 786,\n",
       " 'appear': 787,\n",
       " 'third': 788,\n",
       " 'flat': 789,\n",
       " 'questions': 790,\n",
       " 'strong': 791,\n",
       " 'red': 792,\n",
       " 'trouble': 793,\n",
       " 'released': 794,\n",
       " 'secret': 795,\n",
       " 'likely': 796,\n",
       " 'art': 797,\n",
       " 'familiar': 798,\n",
       " 'sit': 799,\n",
       " 'purpose': 800,\n",
       " 'effect': 801,\n",
       " 'totally': 802,\n",
       " 'blood': 803,\n",
       " 'th': 804,\n",
       " 'told': 805,\n",
       " 'events': 806,\n",
       " 'somewhere': 807,\n",
       " 'whatever': 808,\n",
       " 'large': 809,\n",
       " 'stay': 810,\n",
       " 'means': 811,\n",
       " 'audiences': 812,\n",
       " 'd': 813,\n",
       " 'agent': 814,\n",
       " 'usual': 815,\n",
       " 'million': 816,\n",
       " 'merely': 817,\n",
       " 'julie': 818,\n",
       " 'expected': 819,\n",
       " 'important': 820,\n",
       " 'present': 821,\n",
       " 'ways': 822,\n",
       " 'husband': 823,\n",
       " 'easily': 824,\n",
       " 'happy': 825,\n",
       " 'bruce': 826,\n",
       " 'willis': 827,\n",
       " 'nick': 828,\n",
       " 'disaster': 829,\n",
       " 'points': 830,\n",
       " 'effort': 831,\n",
       " 'using': 832,\n",
       " 'ask': 833,\n",
       " 'felt': 834,\n",
       " 'understand': 835,\n",
       " 'happened': 836,\n",
       " 'successful': 837,\n",
       " 'romance': 838,\n",
       " 'fast': 839,\n",
       " 'catch': 840,\n",
       " 'ryan': 841,\n",
       " 'lots': 842,\n",
       " 'killing': 843,\n",
       " 'write': 844,\n",
       " 'air': 845,\n",
       " 'strange': 846,\n",
       " 'entirely': 847,\n",
       " 'max': 848,\n",
       " 'sam': 849,\n",
       " 'havent': 850,\n",
       " 'biggest': 851,\n",
       " 'box': 852,\n",
       " 'clear': 853,\n",
       " 'thinks': 854,\n",
       " 'directors': 855,\n",
       " 'budget': 856,\n",
       " 'e': 857,\n",
       " 'sounds': 858,\n",
       " 'trailer': 859,\n",
       " 'doubt': 860,\n",
       " 'science': 861,\n",
       " 'spend': 862,\n",
       " 'eye': 863,\n",
       " 'arnold': 864,\n",
       " 'opens': 865,\n",
       " 'due': 866,\n",
       " 'particular': 867,\n",
       " 'unfunny': 868,\n",
       " 'leaving': 869,\n",
       " 'dramatic': 870,\n",
       " 'amusing': 871,\n",
       " 'brings': 872,\n",
       " 'teen': 873,\n",
       " 'pointless': 874,\n",
       " 'clever': 875,\n",
       " 'learn': 876,\n",
       " 'looked': 877,\n",
       " 'company': 878,\n",
       " 'favorite': 879,\n",
       " 'situation': 880,\n",
       " 'christopher': 881,\n",
       " 'heard': 882,\n",
       " 'filled': 883,\n",
       " 'showing': 884,\n",
       " 'uses': 885,\n",
       " 'near': 886,\n",
       " 'otherwise': 887,\n",
       " 'stone': 888,\n",
       " 'ultimately': 889,\n",
       " 'sadly': 890,\n",
       " 'immediately': 891,\n",
       " 'took': 892,\n",
       " 'spent': 893,\n",
       " 'prison': 894,\n",
       " 'previous': 895,\n",
       " 'slow': 896,\n",
       " 'club': 897,\n",
       " 'cage': 898,\n",
       " 'r': 899,\n",
       " 'ben': 900,\n",
       " 'exciting': 901,\n",
       " 'herself': 902,\n",
       " 'knew': 903,\n",
       " 'motion': 904,\n",
       " 'members': 905,\n",
       " 'drug': 906,\n",
       " 'development': 907,\n",
       " 'surprisingly': 908,\n",
       " 'cinema': 909,\n",
       " 'break': 910,\n",
       " 'pull': 911,\n",
       " 'mark': 912,\n",
       " 'aliens': 913,\n",
       " 'horrible': 914,\n",
       " 'change': 915,\n",
       " 'street': 916,\n",
       " 'suddenly': 917,\n",
       " 'screenwriter': 918,\n",
       " 'youd': 919,\n",
       " 'whom': 920,\n",
       " 'humans': 921,\n",
       " 'smart': 922,\n",
       " 'poorly': 923,\n",
       " 'partner': 924,\n",
       " 't': 925,\n",
       " 'somewhat': 926,\n",
       " 'meanwhile': 927,\n",
       " 'road': 928,\n",
       " 'etc': 929,\n",
       " 'create': 930,\n",
       " 'okay': 931,\n",
       " 'loud': 932,\n",
       " 'front': 933,\n",
       " 'gave': 934,\n",
       " 'hear': 935,\n",
       " 'viewer': 936,\n",
       " 'monster': 937,\n",
       " 'college': 938,\n",
       " 'steven': 939,\n",
       " 'situations': 940,\n",
       " 'gags': 941,\n",
       " 'liked': 942,\n",
       " 'wish': 943,\n",
       " 'hands': 944,\n",
       " 'crap': 945,\n",
       " 'open': 946,\n",
       " 'recently': 947,\n",
       " 'low': 948,\n",
       " 'include': 949,\n",
       " 'martin': 950,\n",
       " 'realize': 951,\n",
       " 'involves': 952,\n",
       " 'actual': 953,\n",
       " 'b': 954,\n",
       " 'park': 955,\n",
       " 'tone': 956,\n",
       " 'towards': 957,\n",
       " 'focus': 958,\n",
       " 'possibly': 959,\n",
       " 'moves': 960,\n",
       " 'further': 961,\n",
       " 'please': 962,\n",
       " 'jennifer': 963,\n",
       " 'nowhere': 964,\n",
       " 'appearance': 965,\n",
       " 'control': 966,\n",
       " 'island': 967,\n",
       " 'bland': 968,\n",
       " 'light': 969,\n",
       " 'charm': 970,\n",
       " 'offers': 971,\n",
       " 'hate': 972,\n",
       " 'married': 973,\n",
       " 'remake': 974,\n",
       " 'pathetic': 975,\n",
       " 'wrote': 976,\n",
       " 'various': 977,\n",
       " 'weak': 978,\n",
       " 'amazing': 979,\n",
       " 'quick': 980,\n",
       " 'rich': 981,\n",
       " 'follow': 982,\n",
       " 'tale': 983,\n",
       " 'hold': 984,\n",
       " 'subject': 985,\n",
       " 'follows': 986,\n",
       " 'spawn': 987,\n",
       " 'incredibly': 988,\n",
       " 'free': 989,\n",
       " 'inside': 990,\n",
       " 'hardly': 991,\n",
       " 'opportunity': 992,\n",
       " 'element': 993,\n",
       " 'bill': 994,\n",
       " 'intelligence': 995,\n",
       " 'history': 996,\n",
       " 'rating': 997,\n",
       " 'trip': 998,\n",
       " 'visual': 999,\n",
       " 'doctor': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "# check the result\n",
    "tokenizer.word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viii. Select a review length L that 70% of the reviews have a length below it. If\n",
    "you feel more adventurous, set the threshold to 90%.\n",
    "\n",
    "\n",
    "ix. Truncate reviews longer than L words and zero-pad reviews shorter than L\n",
    "so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at the 70th percentile of review lengths: 700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# viii\n",
    "review_lengths = df['review_length'].values\n",
    "review_70_length = int(np.percentile(review_lengths, 70))\n",
    "print(\"Value at the 70th percentile of review lengths:\", review_70_length)\n",
    "\n",
    "# ix\n",
    "def truncate(texts, tokenizer, max_sequence_length):\n",
    "    # Turns text into into padded sequences.\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # padding='post'\n",
    "    # truncating='post'\n",
    "    # padding is done with zeros at the end of sequences shorter than the specified maximum length \n",
    "    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# here max_sequence_length = percentile 70 (737)\n",
    "text_df = truncate(df[\"text\"], tokenizer, review_70_length)\n",
    "training_x = truncate(training[\"text\"], tokenizer, review_70_length)\n",
    "testing_x = truncate(testing[\"text\"], tokenizer, review_70_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 700)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape\n",
    "\n",
    "# here we have 2000 document (text), and each document's length is 700 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 700)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Word Embeddings\n",
    "\n",
    "use a word embedding layer for this project. Assume that we are interested in the top 5,000 words. This means that in each integer sequence that\n",
    "represents each document, we set to zero those integers that represent words\n",
    "that are not among the top 5,000 words in the document.5 If you feel more\n",
    "adventurous, use all the words that appear in this corpus. Choose the length\n",
    "of the embedding vector for each word to be 32. Hence, each document is\n",
    "represented as a 32 ×L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluation(ytrue, ypred):\n",
    "    # for report result\n",
    "    results = {\n",
    "        'accuracy': [accuracy_score(ytrue, ypred)],\n",
    "        'precision': [precision_score(ytrue, ypred, zero_division=0)],\n",
    "        'recall': [recall_score(ytrue, ypred)],\n",
    "        'f1': [f1_score(ytrue, ypred)],\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents, top_words, embedding_dim):\n",
    "    # Create a dictionary to map words to indices\n",
    "    word_index = {}\n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            word_index[word] = word_index.get(word, 0) + 1\n",
    "            \n",
    "    # Select the top 5000 words based on frequency\n",
    "    top_words_list = sorted(word_index, key=word_index.get, reverse=True)[:top_words]\n",
    "    # Create a word to index dictionary\n",
    "    word_to_index = {word: idx + 1 for idx, word in enumerate(top_words_list)}\n",
    "    # Convert documents to sequences of indices\n",
    "    sequences = [[word_to_index.get(word, 0) for word in doc] for doc in documents]\n",
    "\n",
    "    # Create the embedding matrix\n",
    "    embedding_matrix = np.zeros((top_words + 1, embedding_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx > top_words:\n",
    "            continue\n",
    "        embedding_vector = np.random.rand(embedding_dim)  # initialize with random values or zeros\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    # Create the embedding layer\n",
    "    embedding_layer = Embedding(input_dim=top_words+1, output_dim=embedding_dim,\n",
    "                                weights=[embedding_matrix], input_length=review_70_length)\n",
    "\n",
    "    # Flatten the embedding layer\n",
    "    flatten_layer = Flatten()\n",
    "\n",
    "    return embedding_layer, flatten_layer\n",
    "\n",
    "# Example usage\n",
    "documents = text_df  # shape=(2000, 737)\n",
    "top_words = 5000\n",
    "embedding_dim = 32\n",
    "\n",
    "embedding_layer, flatten_layer = preprocess_documents(documents, top_words, embedding_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Multi-Layer Perceptron\n",
    "\n",
    "Train a MLP with three (dense) hidden layers each of which has 50 ReLUs\n",
    "and one output layer with a single sigmoid neuron. Use a dropout rate of\n",
    "20% for the first layer and 50% for the other layers. Use ADAM optimizer\n",
    "and binary cross entropy loss (which is equivalent to having a softmax in the\n",
    "output). To avoid overfitting, just set the number of epochs as 2. Use a batch\n",
    "size of 10\n",
    "\n",
    "reference: https://keras-cn.readthedocs.io/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[input] -> | hidden layer | -> 50 ReLUS -> | hidden layer | -> 50 ReLUS -> | hidden layer | -> 50 ReLUS -> |output layer (single sigmoid)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "max_words = 5000  # Specify the input dimension\n",
    "epochs = 2\n",
    "batch_size = 10\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(training[\"text\"])\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training[\"text\"])\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing[\"text\"])\n",
    "\n",
    "training_x = sequence.pad_sequences(training_sequences, maxlen=review_70_length, padding='post', truncating='post')\n",
    "testing_x = sequence.pad_sequences(testing_sequences, maxlen=review_70_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9843 - loss: 0.0364\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "training set result:    accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "test set result:    accuracy  precision  recall   f1\n",
      "0       1.0        1.0     1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "''' \n",
    "def bipolar_sigmoid(x):\n",
    "    return (2 / (1 + tf.exp(-x))) - 1\n",
    "\n",
    "def custom_binary_crossentropy_with_labels(y_true, y_pred):\n",
    "    # convert y_true from [-1, 1] to [0, 1]\n",
    "    y_pred = (y_pred + 1) / 2\n",
    "    y_true = (y_true + 1) / 2\n",
    "    \n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())  # 避免log(0)\n",
    "    loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "'''\n",
    "# define model\n",
    "def create_mlp():\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # add the embedding layer\n",
    "    model.add(Embedding(max_words + 1, output_dim=32, input_length = review_70_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Create the MLP model\n",
    "model = create_mlp()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# adjust the label setting since the binary_crossentropy is for 0 / 1 label \n",
    "# or use bipolar_sigmoid and custom binary cross_entropy\n",
    "# Here I chose to adjust the label type(a more simple and easier way)\n",
    "training['label'] = training['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "testing['label'] = testing['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(training_x, training['label'].values, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size)\n",
    "\n",
    "predic = model.predict(training_x, batch_size=128)\n",
    "predictions_binary = [1 if p > 0.5 else -1 for p in predic]\n",
    "training_result = evaluation(training['label'].values, predictions_binary)\n",
    "print(f'training set result: {training_result}')\n",
    "\n",
    "predic = model.predict(testing_x, batch_size=128)\n",
    "predictions_binary = [1 if p > 0.5 else -1 for p in predic]\n",
    "test_result = evaluation(testing['label'].values, predictions_binary)\n",
    "print(f'test set result: {test_result}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) One-Dimensional Convolutional Neural Network\n",
    "\n",
    "i. After the embedding layer, insert a Conv1D layer. This convolutional layer\n",
    "has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded\n",
    "word representations 3 vector elements of the word embedding at a time. The\n",
    "convolutional layer is followed by a 1D max pooling layer with a length and\n",
    "stride of 2 that halves the size of the feature maps from the convolutional\n",
    "layer. The rest of the network is the same as the neural network above.\n",
    "\n",
    "ii. Report the train and test accuracies of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9804 - loss: 0.0323 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "the training accuracy[epoch1]: 99.64%\n",
      "the testing accuracy[epoch1]: 100.00%\n",
      "the training accuracy[epoch2]: 100.00%\n",
      "the testing accuracy[epoch2]: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(max_words + 1, output_dim=32, input_length=review_70_length))\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    model.add(Conv1D(32, 3, activation='relu', padding = 'same'))  # 32 feature maps and a kernel size of 3\n",
    "    # 1D max pooling\n",
    "    model.add(MaxPooling1D(2, padding = 'same'))  # Pool size and strides of 2\n",
    "\n",
    "    # Flatten data\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    return model\n",
    "\n",
    "model = create_cnn()\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss='binary_crossentropy', # or custom_binary_crossentropy_with_labels \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "# Adjust the label setting since the binary_crossentropy is for 0 / 1 label\n",
    "training['label'] = training['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "testing['label'] = testing['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "'''\n",
    "\n",
    "# training and record the history\n",
    "history = model.fit(training_x, training['label'].values, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(testing_x, testing['label'].values))\n",
    "\n",
    "# evaluation\n",
    "train_acc1 = history.history['accuracy'][-2]\n",
    "test_acc1 = history.history['val_accuracy'][-2]\n",
    "print(\"the training accuracy[epoch1]: {:.2f}%\".format(train_acc1 * 100))\n",
    "print(\"the testing accuracy[epoch1]: {:.2f}%\".format(test_acc1 * 100))\n",
    "\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "test_acc = history.history['val_accuracy'][-1]\n",
    "print(\"the training accuracy[epoch2]: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"the testing accuracy[epoch2]: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Long Short-Term Memory Recurrent Neural Network\n",
    "\n",
    "Each word is represented to LSTM as a vector of 32 elements and the LSTM\n",
    "is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch\n",
    "size of 10.\n",
    "\n",
    "Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.9647 - loss: 0.0633 - val_accuracy: 1.0000 - val_loss: 9.4108e-24\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 9.7331e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 1.5608e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 2.4825e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 1.9002e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 7.8655e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 4.0632e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 1.0999e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 5.2768e-22 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 1.8743e-21 - val_accuracy: 1.0000 - val_loss: 9.4102e-24\n",
      "Training Accuracy: 100.00%\n",
      "Testing Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "def create_lstm():\n",
    "    model = keras.models.Sequential()\n",
    "    # embedding layer\n",
    "    model.add(Embedding(max_words + 1, output_dim=32, input_length=review_70_length))\n",
    "    model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))  # 32 LSTM units, add dropout to LSTM\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "model = create_lstm()\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training['label'] = training['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "testing['label'] = testing['label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(training_x, training['label'].values, \n",
    "                    epochs=10, \n",
    "                    batch_size=10,\n",
    "                    validation_data=(testing_x, testing['label'].values))\n",
    "\n",
    "# evaluation\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "test_acc = history.history['val_accuracy'][-1]\n",
    "print(\"the training accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"the testing accuracy: {:.2f}%\".format(test_acc * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
